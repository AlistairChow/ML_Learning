{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯分类器（Naive Bayesian Classifier）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "朴素贝叶斯是基于贝叶斯，定理与特征条件独立假设的分类方法。最为广泛的两种分类模型是决策树模型和朴素贝叶斯模型。  \n",
    "和决策树模型相比，朴素贝叶斯分类器(Naive Bayesian Classifier, NBC)发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比，具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这个NBC模型的正确分类带来了一定影响。  \n",
    "> **优点**：在数据较少的情况下任然有效，可以处理多类别问题  \n",
    "> **缺点**：对于输入数据的准备方式较为敏感  \n",
    "> **使用数据类型**：标称型数据  \n",
    "\n",
    "贝叶斯决策理论的核心思想是，选择具有最高概率的决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法流程\n",
    "1. 收集数据：可以使用任何方法。  \n",
    "2. 准备数据：需要数值型或者布尔型数据\n",
    "3. 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "4. 训练算法：计算不同的独立特征的条件概率\n",
    "5. 测试算法：计算错误率\n",
    "6. 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：从文本中构建词向量\n",
    "将文本看成单词向量或词条向量，也就是说把句子转换为向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    '''\n",
    "    构造样本数据\n",
    "    '''\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    #  1：代表侮辱性文字， 0：代表正常言论\n",
    "    classVec = [0, 1, 0, 1, 0, 1] \n",
    "    return postingList, classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    '''\n",
    "    创建文本中单词列表\n",
    "    '''\n",
    "    vocabSet = set([])\n",
    "    \n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    \n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    '''\n",
    "    单词是否在文档中出现，出现设为1，不出现为0\n",
    "    param vocabList: 单词列表\n",
    "    param inputSet: 输入文本\n",
    "    '''\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    \n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print 'the word: %s is not in my Vocabulary!' % word\n",
    "    \n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute',\n",
       " 'love',\n",
       " 'help',\n",
       " 'garbage',\n",
       " 'quit',\n",
       " 'I',\n",
       " 'problems',\n",
       " 'is',\n",
       " 'park',\n",
       " 'stop',\n",
       " 'flea',\n",
       " 'dalmation',\n",
       " 'licks',\n",
       " 'food',\n",
       " 'not',\n",
       " 'him',\n",
       " 'buying',\n",
       " 'posting',\n",
       " 'has',\n",
       " 'worthless',\n",
       " 'ate',\n",
       " 'to',\n",
       " 'maybe',\n",
       " 'please',\n",
       " 'dog',\n",
       " 'how',\n",
       " 'stupid',\n",
       " 'so',\n",
       " 'take',\n",
       " 'mr',\n",
       " 'steak',\n",
       " 'my']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：从词向量计算概率\n",
    "贝叶斯准则：\n",
    "$$p(c_i|\\boldsymbol{w})=\\frac{p(\\boldsymbol{w}|c_i)p(c_i)}{p(\\boldsymbol{w})}$$\n",
    "$\\boldsymbol{w}$表示这是一个向量，即它由多个数值组成。$\\boldsymbol{w}$中元素众多，使用Numpy数组快速计算这些值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategroy):\n",
    "    '''\n",
    "    朴素贝叶斯分类器训练函数\n",
    "    param trainMatrix: 文档矩阵\n",
    "    param trainCategory: 文档类别标签向量\n",
    "    return p0Num: 正常言论概率向量\n",
    "    return p1Num: 侮辱性言论概率向量\n",
    "    return pAbusive: 侮辱性文档概率向量\n",
    "    '''\n",
    "    # 文档数量\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 单词数量\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性语句概率(侮辱性语句数量除以语句总数)\n",
    "    pAbusive = sum(trainCategroy)/float(numTrainDocs)\n",
    "    # 各单词出现向量初始化\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    \n",
    "    # 遍历文档矩阵\n",
    "    for i in range(numTrainDocs):\n",
    "        # 判定文档所对应的分类，并对该分类向量进行累加\n",
    "        if trainCategroy[i] == 1:            \n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "            \n",
    "    # 侮辱性言论，单词概率向量（各单词出现次数除以单词总量）\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    # 正常言论，单词概率向量\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对样本数据集进行朴素贝叶斯分类，得到出现侮辱性语言的概率为0.5。  \n",
    "从样本数据中可以看到，总共有6句话，有三句是侮辱性语句，因此概率0.5是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载样本数据集\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "# 单词列表\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat = []\n",
    "# 遍历样本数据集\n",
    "for postinDoc in listOPosts:\n",
    "    # 将文本转换为单词向量\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看侮辱性言论中各单词出现的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.05263158,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "        0.        ,  0.05263158,  0.05263158,  0.        ,  0.10526316,\n",
       "        0.        ,  0.15789474,  0.        ,  0.05263158,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找出侮辱性言论中单词出现概率最大的值和其对应的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15789473684210525, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V.max(), p1V.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词列表中找到对应索引的单词，发现该单词为'stupid'。这意味着'stupid'是最能表征侮辱性言论类别的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stupid'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：根据现实情况修改分类器\n",
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$p(w_0|1)p(w_1|1)p(w_2|1)$。如果其中一个概率为0，那么最后的乘积也为0。  \n",
    "为了降低这种影响，可以将所有词出现数初始化为1，并将分母初始化为2。\n",
    "\n",
    "另一个问题是下溢出，这是由于太多很小的数相乘造成的。由于大部分因子都非常小，所以程序会下溢出或者得不到正确答案。  \n",
    "一种解决办法是对乘积取自然对数。在代数中有$ln(a*b)=ln(a)+ln(b)$，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategroy):\n",
    "    '''\n",
    "    朴素贝叶斯分类器训练函数\n",
    "    param trainMatrix: 文档矩阵\n",
    "    param trainCategory: 文档类别标签向量\n",
    "    return p0Num: 正常言论概率向量\n",
    "    return p1Num: 侮辱性言论概率向量\n",
    "    return pAbusive: 侮辱性文档概率向量\n",
    "    '''\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategroy)/float(numTrainDocs)\n",
    "    # 各单词出现向量初始化为1\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    # 分母初始化为2\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategroy[i] == 1:            \n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "\n",
    "    # 修改为取对数\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    '''\n",
    "    朴素贝叶斯分类函数\n",
    "    param vec2Classify: 要分类的向量\n",
    "    param p0Vec: 正常言论单词概率向量\n",
    "    param p1Vec: 侮辱性言论单词概率向量\n",
    "    param pClass1: 侮辱性言论概率\n",
    "    '''\n",
    "    # 单词出现概率和 + 分类概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    \n",
    "    # 返回概率大的类别\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    # 训练朴素贝叶斯分类器\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    \n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    \n",
    "    # 测试朴素贝叶斯分类器\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    \n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：文档词袋模型\n",
    "上面将每个单词在文本中出现与否作为一个特征，这可以被描述为词集模型（set-of-words model）。  \n",
    "如果一个词在文档中出现不止一次，这可能意味着该词是否出现在文档中不能表达的某种信息，这种方法被称为词袋模型（bag-of-words model)。  \n",
    "词袋中每个单词可以出现多次，而词集中每个单词只能出现一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfwords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    \n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：电子邮件垃圾过滤\n",
    "> **收集数据**：提供文本文件  \n",
    "> **准备数据**：将文本文件解析成词条向量  \n",
    "> **分析数据**；检查词条确保解析的正确性  \n",
    "> **训练算法**：使用之前建立的trainNB0()函数  \n",
    "> **测试算法**：使用classifyNB()，并且构建一个新的测试函数来计算文档集的错误率  \n",
    "> **使用算法**：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据：切分文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用正则表达式切分，其中分隔符是除单词、数字外的任意字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "# 去掉长度小于0的单词，并转换为小写\n",
    "[tok.lower() for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法：使用朴素贝叶斯进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def textParse(bigString):\n",
    "    '''\n",
    "    字符串解析\n",
    "    '''\n",
    "    import re\n",
    "    # 根据非数字字母的任意字符进行拆分\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    # 拆分后字符串长度大于2的字符串，并转换为小写\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    '''\n",
    "    贝叶斯分类器对垃圾邮件进行自动化处理\n",
    "    '''\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    \n",
    "    for i in range(1, 26):\n",
    "        # 读取spam文件夹下的文件，并转换为特征和标签向量\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        # 读取ham文件夹下的文件，并转换为特征和标签向量\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    # 转换为词列表\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 初始化训练集和测试集\n",
    "    trainingSet = range(50);\n",
    "    testSet = []\n",
    "    \n",
    "    # 随机抽取测试集索引\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    \n",
    "    # 构造训练集\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "        \n",
    "    # 朴素贝叶斯分类模型训练\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    \n",
    "    # 朴素贝叶斯分类模型测试\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print 'classification error', docList[docIndex]\n",
    "    \n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于SpamTest()构造的测试集和训练集是随机的，所以每次运行的分类结果可能不一样。如果发生错误，函数会输出错分文档的词表，这样就可以了解到底哪篇文档发生了错误。  \n",
    "这里出现的错误是将垃圾邮件误判为了正常邮件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向\n",
    "分别从美国的两个城市中选取一些人，通过这些人发布的征婚广告信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么各自的常用词有哪些？从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解？\n",
    "> 收集数据：从RSS源收集内容  \n",
    "> 准备数据：将文本解析成词条向量  \n",
    "> 分析数据：检查词条以确保词条的正确性  \n",
    "> 训练算法：使用之前建立的traingNB0()函数  \n",
    "> 测试算法：观察错误率，确保分类器可用。可以修改切片程序，以降低错误率，提高分类结果  \n",
    "> 使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示常用的公共词  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 收集数据：导入RSS源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import feedparser\n",
    "\n",
    "def calcMostFreq(vocabList, fullText):\n",
    "    '''\n",
    "    返回前30个频率最高的单词\n",
    "    '''\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "        \n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1, feed0):\n",
    "    '''\n",
    "    RSS分类器\n",
    "    '''\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    \n",
    "    # 获取两个RSS源最小条目数\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    \n",
    "    # 解析RSS内容，并转换为特征和标签向量\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "        \n",
    "    # 转换为词列表\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 词列表中移除频度出现最高的前30个单词\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList:\n",
    "            vocabList.remove(pairW[0])\n",
    "    \n",
    "    trainingSet = range(2*minLen)\n",
    "    testSet = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "        \n",
    "    # 构造训练集\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfwords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \n",
    "    # 训练模型\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    # 测试模型\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfwords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    \n",
    "    print 'the error rate is: ', float(errorCount)/len(testSet)\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证RSS分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.45\n"
     ]
    }
   ],
   "source": [
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocabList, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据：显示地域相关的用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    '''\n",
    "    显示最具表征性的词汇\n",
    "    '''\n",
    "    import operator\n",
    "    # 训练并测试朴素贝叶斯分类器\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY = []\n",
    "    topSF = []\n",
    "    \n",
    "    # 将概率大于-6.0的单词加入列表\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0:\n",
    "            topSF.append((vocabList[i], p0V[i]))\n",
    "        \n",
    "        if p1V[i] > -6.0:\n",
    "            topNY.append((vocabList[i], p1V[i]))\n",
    "            \n",
    "    # 倒序排列并返回\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print 'SF**'*14\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    \n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print 'NY**'*14\n",
    "    for item in sortedNY:\n",
    "        print item[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.4\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "love\n",
      "seeking\n",
      "naked\n",
      "down\n",
      "all\n",
      "thursday\n",
      "says\n",
      "great\n",
      "from\n",
      "few\n",
      "girl\n",
      "got\n",
      "dont\n",
      "friend\n",
      "relationship\n",
      "sex\n",
      "whatever\n",
      "doesn\n",
      "coffee\n",
      "well\n",
      "contact\n",
      "hang\n",
      "there\n",
      "was\n",
      "happy\n",
      "really\n",
      "friends\n",
      "asian\n",
      "hanging\n",
      "monty\n",
      "relationships\n",
      "flash\n",
      "compensate\n",
      "under\n",
      "include\n",
      "woman\n",
      "fall\n",
      "difference\n",
      "school\n",
      "respectful\n",
      "clothed\n",
      "try\n",
      "small\n",
      "tendril\n",
      "challenge\n",
      "healer\n",
      "drinks\n",
      "blue\n",
      "current\n",
      "new\n",
      "full\n",
      "hours\n",
      "healing\n",
      "alone\n",
      "kids\n",
      "makes\n",
      "hiking\n",
      "two\n",
      "live\n",
      "bike\n",
      "music\n",
      "posting\n",
      "glass\n",
      "women\n",
      "town\n",
      "room\n",
      "this\n",
      "ride\n",
      "laughter\n",
      "crazy\n",
      "share\n",
      "partner\n",
      "times\n",
      "travel\n",
      "intelligent\n",
      "man\n",
      "stress\n",
      "maybe\n",
      "talk\n",
      "wine\n",
      "over\n",
      "soon\n",
      "still\n",
      "possessiv\n",
      "before\n",
      "glen\n",
      "willow\n",
      "interesting\n",
      "them\n",
      "food\n",
      "break\n",
      "lunch\n",
      "now\n",
      "adventurous\n",
      "drop\n",
      "did\n",
      "strip\n",
      "used\n",
      "year\n",
      "silky\n",
      "laid\n",
      "routine\n",
      "forgive\n",
      "route\n",
      "thing\n",
      "open\n",
      "district\n",
      "divorce\n",
      "remotely\n",
      "wonderful\n",
      "too\n",
      "undies\n",
      "listen\n",
      "interests\n",
      "hug\n",
      "hotel\n",
      "wide\n",
      "kind\n",
      "light\n",
      "bimwm\n",
      "any\n",
      "nights\n",
      "mostly\n",
      "convenient\n",
      "breeze\n",
      "trivia\n",
      "play\n",
      "normal\n",
      "most\n",
      "don\n",
      "definitely\n",
      "clean\n",
      "earth\n",
      "find\n",
      "touches\n",
      "title\n",
      "going\n",
      "hope\n",
      "guarded\n",
      "optimism\n",
      "preferred\n",
      "feels\n",
      "married\n",
      "stuff\n",
      "activity\n",
      "smooth\n",
      "best\n",
      "visiting\n",
      "movie\n",
      "nature\n",
      "please\n",
      "heading\n",
      "horrible\n",
      "enjoys\n",
      "job\n",
      "come\n",
      "last\n",
      "otherwise\n",
      "tough\n",
      "arts\n",
      "been\n",
      "interest\n",
      "meeting\n",
      "educated\n",
      "shopping\n",
      "lift\n",
      "straight\n",
      "will\n",
      "smart\n",
      "fun\n",
      "hoping\n",
      "sincere\n",
      "middle\n",
      "against\n",
      "different\n",
      "etc\n",
      "perhaps\n",
      "events\n",
      "jealous\n",
      "collar\n",
      "off\n",
      "executive\n",
      "cut\n",
      "interested\n",
      "attractive\n",
      "crack\n",
      "showing\n",
      "possible\n",
      "possibly\n",
      "helpful\n",
      "buddies\n",
      "because\n",
      "old\n",
      "back\n",
      "passions\n",
      "creative\n",
      "business\n",
      "gentleman\n",
      "anything\n",
      "slender\n",
      "mutually\n",
      "sweat\n",
      "pays\n",
      "female\n",
      "fabric\n",
      "her\n",
      "area\n",
      "stuck\n",
      "lot\n",
      "forward\n",
      "buy\n",
      "younger\n",
      "hear\n",
      "line\n",
      "trying\n",
      "attached\n",
      "peeking\n",
      "affection\n",
      "trim\n",
      "politics\n",
      "again\n",
      "variety\n",
      "lately\n",
      "polite\n",
      "spouse\n",
      "assets\n",
      "age\n",
      "required\n",
      "hello\n",
      "edc\n",
      "dynamic\n",
      "results\n",
      "edm\n",
      "miyutzdigoolkin\n",
      "pantywaist\n",
      "shoobunzaaaaah\n",
      "buddy\n",
      "religion\n",
      "young\n",
      "good\n",
      "answered\n",
      "gwazoozy\n",
      "friendly\n",
      "koozya\n",
      "putting\n",
      "exact\n",
      "encounter\n",
      "cool\n",
      "dim\n",
      "level\n",
      "marry\n",
      "nostrand\n",
      "sane\n",
      "enjoy\n",
      "clients\n",
      "elegant\n",
      "crave\n",
      "what\n",
      "curse\n",
      "sub\n",
      "movies\n",
      "suspect\n",
      "ever\n",
      "burmunzay\n",
      "simultaneously\n",
      "exchange\n",
      "men\n",
      "explore\n",
      "let\n",
      "situation\n",
      "sexy\n",
      "strong\n",
      "thirty\n",
      "experience\n",
      "pics\n",
      "smoke\n",
      "stressed\n",
      "usually\n",
      "massage\n",
      "extra\n",
      "prefer\n",
      "private\n",
      "ask\n",
      "send\n",
      "use\n",
      "fee\n",
      "dude\n",
      "bdsm\n",
      "cannabis\n",
      "type\n",
      "tell\n",
      "today\n",
      "more\n",
      "sort\n",
      "fleebumdimoomble\n",
      "sessions\n",
      "started\n",
      "hurt\n",
      "train\n",
      "basically\n",
      "baby\n",
      "keeping\n",
      "join\n",
      "hour\n",
      "work\n",
      "sucks\n",
      "offended\n",
      "cuddle\n",
      "incidents\n",
      "cam\n",
      "male\n",
      "could\n",
      "shook\n",
      "control\n",
      "give\n",
      "process\n",
      "confused\n",
      "ready\n",
      "minimum\n",
      "something\n",
      "cultivate\n",
      "clothing\n",
      "preferably\n",
      "winter\n",
      "dinners\n",
      "watching\n",
      "magazine\n",
      "write\n",
      "how\n",
      "hugs\n",
      "nyc\n",
      "bodyrub\n",
      "advice\n",
      "after\n",
      "lighting\n",
      "date\n",
      "suck\n",
      "prostate\n",
      "cuddling\n",
      "rather\n",
      "things\n",
      "make\n",
      "trade\n",
      "its\n",
      "inquiries\n",
      "group\n",
      "expectations\n",
      "weeks\n",
      "might\n",
      "into\n",
      "then\n",
      "evening\n",
      "somebody\n",
      "safe\n",
      "kiss\n",
      "hands\n",
      "urbunflay\n",
      "name\n",
      "anastasia\n",
      "each\n",
      "everyone\n",
      "doing\n",
      "energy\n",
      "wednesday\n",
      "our\n",
      "bodywork\n",
      "special\n",
      "tensions\n",
      "since\n",
      "race\n",
      "cause\n",
      "bubba\n",
      "humiliating\n",
      "free\n",
      "worries\n",
      "put\n",
      "conversationalist\n",
      "honest\n",
      "isn\n",
      "first\n",
      "platonic\n",
      "feel\n",
      "relate\n",
      "parkinson\n",
      "yourself\n",
      "fast\n",
      "ring\n",
      "size\n",
      "anyone\n",
      "little\n",
      "long\n",
      "boozya\n",
      "girls\n",
      "stimulation\n",
      "final\n",
      "doozya\n",
      "way\n",
      "rubbing\n",
      "nearby\n",
      "qual\n",
      "weightlifting\n",
      "than\n",
      "breaker\n",
      "lmao\n",
      "double\n",
      "bed\n",
      "hurkazoolitz\n",
      "matter\n",
      "didn\n",
      "floopunbaaaaah\n",
      "str\n",
      "san\n",
      "modern\n",
      "comfortable\n",
      "talking\n",
      "hangout\n",
      "sensual\n",
      "nipples\n",
      "genuine\n",
      "mid\n",
      "dihoomble\n",
      "also\n",
      "ideal\n",
      "take\n",
      "which\n",
      "towards\n",
      "brooklyn\n",
      "regular\n",
      "experiences\n",
      "model\n",
      "queens\n",
      "muscle\n",
      "especially\n",
      "medical\n",
      "drive\n",
      "pictures\n",
      "professional\n",
      "saying\n",
      "walking\n",
      "latinos\n",
      "text\n",
      "random\n",
      "discovered\n",
      "session\n",
      "heerminzozo\n",
      "one\n",
      "busy\n",
      "appreciated\n",
      "athletic\n",
      "pretty\n",
      "employed\n",
      "local\n",
      "unbeatable\n",
      "watch\n",
      "unhurried\n",
      "breast\n",
      "lighter\n",
      "reply\n",
      "during\n",
      "him\n",
      "bad\n",
      "release\n",
      "where\n",
      "btw\n",
      "concert\n",
      "humiliation\n",
      "physically\n",
      "see\n",
      "college\n",
      "subject\n",
      "shower\n",
      "expert\n",
      "lots\n",
      "degrading\n",
      "email\n",
      "available\n",
      "recently\n",
      "missing\n",
      "pained\n",
      "lotion\n",
      "cuddler\n",
      "fleekibobo\n",
      "modeling\n",
      "legit\n",
      "both\n",
      "hits\n",
      "many\n",
      "drug\n",
      "personal\n",
      "sketchy\n",
      "games\n",
      "passing\n",
      "distress\n",
      "gone\n",
      "non\n",
      "towels\n",
      "rave\n",
      "openness\n",
      "indefinite\n",
      "duo\n",
      "three\n",
      "whom\n",
      "expected\n",
      "treatment\n",
      "wants\n",
      "life\n",
      "relaxing\n",
      "offered\n",
      "dates\n",
      "comfor\n",
      "else\n",
      "tangible\n",
      "those\n",
      "chill\n",
      "plants\n",
      "myself\n",
      "these\n",
      "nothingness\n",
      "bikini\n",
      "aid\n",
      "eventually\n",
      "couples\n",
      "tissue\n",
      "baths\n",
      "thug\n",
      "ave\n",
      "around\n",
      "pay\n",
      "35m\n",
      "orthodox\n",
      "craigslist\n",
      "advise\n",
      "week\n",
      "oil\n",
      "emotionally\n",
      "banter\n",
      "drink\n",
      "upon\n",
      "kik\n",
      "student\n",
      "mentally\n",
      "recent\n",
      "thought\n",
      "person\n",
      "qualities\n",
      "consistency\n",
      "entry\n",
      "chemistry\n",
      "spend\n",
      "mon\n",
      "moments\n",
      "money\n",
      "ideas\n",
      "questions\n",
      "using\n",
      "touch\n",
      "tone\n",
      "yes\n",
      "guys\n",
      "jose\n",
      "had\n",
      "rosa\n",
      "day\n",
      "easy\n",
      "match\n",
      "real\n",
      "increased\n",
      "articles\n",
      "shwashagollitz\n",
      "discreet\n",
      "know\n",
      "judge\n",
      "amp\n",
      "bit\n",
      "desire\n",
      "nude\n",
      "fwb\n",
      "sissy\n",
      "payments\n",
      "soft\n",
      "often\n",
      "deal\n",
      "successfully\n",
      "lips\n",
      "sure\n",
      "curious\n",
      "intellect\n",
      "scale\n",
      "humor\n",
      "mood\n",
      "fox\n",
      "everything\n",
      "verbally\n",
      "frum\n",
      "ddf\n",
      "rub\n",
      "fitness\n",
      "plz\n",
      "host\n",
      "nudism\n",
      "worthy\n",
      "netflix\n",
      "choi\n",
      "dinner\n",
      "threesome\n",
      "other\n",
      "seeing\n",
      "own\n",
      "image\n",
      "guburkin\n",
      "accounts\n",
      "heated\n",
      "your\n",
      "weekend\n",
      "strictly\n",
      "xoxo\n",
      "transform\n",
      "lol\n",
      "hardships\n",
      "stamina\n",
      "bored\n",
      "head\n",
      "form\n",
      "offer\n",
      "jewish\n",
      "feminine\n",
      "hmmmmmmm\n",
      "flaky\n",
      "bud\n",
      "handsome\n",
      "places\n",
      "maturity\n",
      "similar\n",
      "clear\n",
      "distance\n",
      "gargamoonchik\n",
      "his\n",
      "deep\n",
      "right\n",
      "chat\n",
      "shit\n",
      "gay\n",
      "dressed\n",
      "power\n",
      "club\n",
      "parlor\n",
      "setting\n",
      "details\n",
      "bassein\n",
      "dresses\n",
      "nice\n",
      "picture\n",
      "sink\n",
      "hey\n",
      "separate\n",
      "felt\n",
      "includes\n",
      "kinda\n",
      "fresh\n",
      "hubby\n",
      "puertorice\n",
      "giburzy\n",
      "together\n",
      "toozya\n",
      "time\n",
      "serious\n",
      "nudists\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "what\n",
      "free\n",
      "massage\n",
      "this\n",
      "travel\n",
      "magazine\n",
      "over\n",
      "platonic\n",
      "dont\n",
      "friend\n",
      "don\n",
      "professional\n",
      "life\n",
      "real\n",
      "other\n",
      "chat\n",
      "time\n",
      "buddy\n",
      "good\n",
      "gwazoozy\n",
      "putting\n",
      "cool\n",
      "full\n",
      "smoke\n",
      "music\n",
      "train\n",
      "could\n",
      "maybe\n",
      "its\n",
      "honest\n",
      "little\n",
      "girls\n",
      "hurkazoolitz\n",
      "hangout\n",
      "sensual\n",
      "most\n",
      "sex\n",
      "fleekibobo\n",
      "myself\n",
      "will\n",
      "craigslist\n",
      "person\n",
      "guys\n",
      "know\n",
      "amp\n",
      "anything\n",
      "seeing\n",
      "there\n",
      "really\n",
      "together\n",
      "all\n",
      "miyutzdigoolkin\n",
      "pantywaist\n",
      "shoobunzaaaaah\n",
      "relationships\n",
      "religion\n",
      "young\n",
      "woman\n",
      "exact\n",
      "dim\n",
      "respectful\n",
      "marry\n",
      "nostrand\n",
      "small\n",
      "crave\n",
      "curse\n",
      "sub\n",
      "movies\n",
      "new\n",
      "ever\n",
      "burmunzay\n",
      "simultaneously\n",
      "exchange\n",
      "men\n",
      "explore\n",
      "let\n",
      "alone\n",
      "strong\n",
      "thirty\n",
      "experience\n",
      "pics\n",
      "stressed\n",
      "extra\n",
      "prefer\n",
      "private\n",
      "send\n",
      "use\n",
      "from\n",
      "live\n",
      "more\n",
      "sort\n",
      "fleebumdimoomble\n",
      "women\n",
      "room\n",
      "hour\n",
      "work\n",
      "offended\n",
      "cam\n",
      "male\n",
      "confused\n",
      "minimum\n",
      "something\n",
      "partner\n",
      "watching\n",
      "write\n",
      "hugs\n",
      "nyc\n",
      "advice\n",
      "lighting\n",
      "date\n",
      "prostate\n",
      "cuddling\n",
      "rather\n",
      "talk\n",
      "group\n",
      "expectations\n",
      "into\n",
      "hands\n",
      "urbunflay\n",
      "each\n",
      "everyone\n",
      "doing\n",
      "year\n",
      "our\n",
      "girl\n",
      "bodywork\n",
      "special\n",
      "race\n",
      "cause\n",
      "humiliating\n",
      "put\n",
      "isn\n",
      "feel\n",
      "size\n",
      "long\n",
      "stimulation\n",
      "too\n",
      "final\n",
      "nearby\n",
      "matter\n",
      "floopunbaaaaah\n",
      "str\n",
      "modern\n",
      "comfortable\n",
      "talking\n",
      "any\n",
      "nights\n",
      "genuine\n",
      "mid\n",
      "dihoomble\n",
      "also\n",
      "which\n",
      "towards\n",
      "brooklyn\n",
      "queens\n",
      "especially\n",
      "saying\n",
      "walking\n",
      "latinos\n",
      "text\n",
      "random\n",
      "discovered\n",
      "session\n",
      "earth\n",
      "find\n",
      "heerminzozo\n",
      "one\n",
      "athletic\n",
      "employed\n",
      "unhurried\n",
      "during\n",
      "married\n",
      "release\n",
      "humiliation\n",
      "college\n",
      "subject\n",
      "shower\n",
      "expert\n",
      "lots\n",
      "movie\n",
      "please\n",
      "degrading\n",
      "missing\n",
      "lotion\n",
      "cuddler\n",
      "modeling\n",
      "legit\n",
      "hits\n",
      "last\n",
      "drug\n",
      "personal\n",
      "towels\n",
      "indefinite\n",
      "expected\n",
      "educated\n",
      "relaxing\n",
      "offered\n",
      "dates\n",
      "comfor\n",
      "tangible\n",
      "chill\n",
      "straight\n",
      "nothingness\n",
      "bikini\n",
      "tissue\n",
      "baths\n",
      "ave\n",
      "around\n",
      "orthodox\n",
      "week\n",
      "oil\n",
      "drink\n",
      "upon\n",
      "student\n",
      "well\n",
      "spend\n",
      "money\n",
      "rosa\n",
      "attractive\n",
      "match\n",
      "articles\n",
      "shwashagollitz\n",
      "discreet\n",
      "judge\n",
      "bit\n",
      "nude\n",
      "sissy\n",
      "payments\n",
      "soft\n",
      "old\n",
      "successfully\n",
      "sure\n",
      "curious\n",
      "everything\n",
      "verbally\n",
      "frum\n",
      "plz\n",
      "host\n",
      "nudism\n",
      "worthy\n",
      "netflix\n",
      "choi\n",
      "own\n",
      "down\n",
      "guburkin\n",
      "accounts\n",
      "female\n",
      "heated\n",
      "hang\n",
      "your\n",
      "strictly\n",
      "was\n",
      "happy\n",
      "offer\n",
      "jewish\n",
      "handsome\n",
      "places\n",
      "attached\n",
      "distance\n",
      "gargamoonchik\n",
      "his\n",
      "deep\n",
      "shit\n",
      "gay\n",
      "dressed\n",
      "parlor\n",
      "setting\n",
      "sink\n",
      "hey\n",
      "separate\n",
      "felt\n",
      "includes\n",
      "fresh\n",
      "puertorice\n",
      "giburzy\n",
      "age\n",
      "serious\n",
      "hello\n",
      "nudists\n",
      "edc\n",
      "dynamic\n",
      "results\n",
      "edm\n",
      "asian\n",
      "hanging\n",
      "monty\n",
      "flash\n",
      "thursday\n",
      "answered\n",
      "compensate\n",
      "under\n",
      "include\n",
      "friendly\n",
      "koozya\n",
      "fall\n",
      "difference\n",
      "encounter\n",
      "school\n",
      "level\n",
      "clothed\n",
      "try\n",
      "sane\n",
      "tendril\n",
      "challenge\n",
      "enjoy\n",
      "clients\n",
      "says\n",
      "elegant\n",
      "healer\n",
      "drinks\n",
      "blue\n",
      "current\n",
      "suspect\n",
      "hours\n",
      "healing\n",
      "situation\n",
      "sexy\n",
      "great\n",
      "kids\n",
      "usually\n",
      "makes\n",
      "love\n",
      "hiking\n",
      "ask\n",
      "fee\n",
      "dude\n",
      "two\n",
      "few\n",
      "bike\n",
      "bdsm\n",
      "cannabis\n",
      "type\n",
      "tell\n",
      "today\n",
      "posting\n",
      "sessions\n",
      "started\n",
      "hurt\n",
      "glass\n",
      "basically\n",
      "baby\n",
      "town\n",
      "keeping\n",
      "join\n",
      "ride\n",
      "sucks\n",
      "cuddle\n",
      "incidents\n",
      "laughter\n",
      "shook\n",
      "control\n",
      "crazy\n",
      "give\n",
      "process\n",
      "share\n",
      "ready\n",
      "cultivate\n",
      "times\n",
      "clothing\n",
      "preferably\n",
      "winter\n",
      "dinners\n",
      "how\n",
      "intelligent\n",
      "bodyrub\n",
      "after\n",
      "suck\n",
      "man\n",
      "stress\n",
      "things\n",
      "make\n",
      "wine\n",
      "soon\n",
      "trade\n",
      "still\n",
      "possessiv\n",
      "inquiries\n",
      "before\n",
      "glen\n",
      "willow\n",
      "interesting\n",
      "weeks\n",
      "might\n",
      "then\n",
      "them\n",
      "evening\n",
      "somebody\n",
      "seeking\n",
      "food\n",
      "safe\n",
      "break\n",
      "lunch\n",
      "kiss\n",
      "now\n",
      "adventurous\n",
      "name\n",
      "drop\n",
      "did\n",
      "anastasia\n",
      "energy\n",
      "strip\n",
      "wednesday\n",
      "used\n",
      "silky\n",
      "tensions\n",
      "since\n",
      "laid\n",
      "got\n",
      "bubba\n",
      "routine\n",
      "forgive\n",
      "worries\n",
      "route\n",
      "conversationalist\n",
      "thing\n",
      "first\n",
      "relate\n",
      "parkinson\n",
      "yourself\n",
      "fast\n",
      "ring\n",
      "open\n",
      "anyone\n",
      "boozya\n",
      "district\n",
      "divorce\n",
      "remotely\n",
      "wonderful\n",
      "undies\n",
      "listen\n",
      "interests\n",
      "hug\n",
      "doozya\n",
      "way\n",
      "relationship\n",
      "rubbing\n",
      "hotel\n",
      "qual\n",
      "weightlifting\n",
      "than\n",
      "breaker\n",
      "wide\n",
      "kind\n",
      "lmao\n",
      "double\n",
      "bed\n",
      "didn\n",
      "light\n",
      "san\n",
      "bimwm\n",
      "nipples\n",
      "mostly\n",
      "convenient\n",
      "ideal\n",
      "breeze\n",
      "take\n",
      "trivia\n",
      "play\n",
      "normal\n",
      "regular\n",
      "experiences\n",
      "model\n",
      "muscle\n",
      "medical\n",
      "drive\n",
      "definitely\n",
      "clean\n",
      "pictures\n",
      "touches\n",
      "busy\n",
      "title\n",
      "appreciated\n",
      "going\n",
      "pretty\n",
      "local\n",
      "hope\n",
      "guarded\n",
      "optimism\n",
      "unbeatable\n",
      "watch\n",
      "preferred\n",
      "feels\n",
      "breast\n",
      "lighter\n",
      "reply\n",
      "him\n",
      "naked\n",
      "bad\n",
      "stuff\n",
      "activity\n",
      "where\n",
      "btw\n",
      "concert\n",
      "physically\n",
      "smooth\n",
      "see\n",
      "best\n",
      "visiting\n",
      "nature\n",
      "heading\n",
      "horrible\n",
      "enjoys\n",
      "email\n",
      "available\n",
      "recently\n",
      "pained\n",
      "job\n",
      "come\n",
      "both\n",
      "many\n",
      "sketchy\n",
      "games\n",
      "passing\n",
      "otherwise\n",
      "tough\n",
      "distress\n",
      "gone\n",
      "whatever\n",
      "non\n",
      "rave\n",
      "openness\n",
      "arts\n",
      "duo\n",
      "three\n",
      "been\n",
      "whom\n",
      "interest\n",
      "meeting\n",
      "treatment\n",
      "wants\n",
      "shopping\n",
      "else\n",
      "lift\n",
      "doesn\n",
      "those\n",
      "plants\n",
      "these\n",
      "smart\n",
      "fun\n",
      "aid\n",
      "hoping\n",
      "eventually\n",
      "sincere\n",
      "coffee\n",
      "couples\n",
      "middle\n",
      "against\n",
      "thug\n",
      "different\n",
      "etc\n",
      "perhaps\n",
      "pay\n",
      "35m\n",
      "advise\n",
      "events\n",
      "emotionally\n",
      "banter\n",
      "kik\n",
      "jealous\n",
      "collar\n",
      "mentally\n",
      "recent\n",
      "off\n",
      "thought\n",
      "contact\n",
      "qualities\n",
      "consistency\n",
      "entry\n",
      "chemistry\n",
      "mon\n",
      "moments\n",
      "executive\n",
      "ideas\n",
      "questions\n",
      "using\n",
      "touch\n",
      "tone\n",
      "yes\n",
      "jose\n",
      "cut\n",
      "had\n",
      "day\n",
      "interested\n",
      "easy\n",
      "crack\n",
      "increased\n",
      "showing\n",
      "possible\n",
      "possibly\n",
      "helpful\n",
      "desire\n",
      "buddies\n",
      "fwb\n",
      "because\n",
      "often\n",
      "deal\n",
      "back\n",
      "lips\n",
      "intellect\n",
      "passions\n",
      "scale\n",
      "humor\n",
      "mood\n",
      "fox\n",
      "creative\n",
      "ddf\n",
      "business\n",
      "rub\n",
      "gentleman\n",
      "fitness\n",
      "slender\n",
      "mutually\n",
      "dinner\n",
      "sweat\n",
      "threesome\n",
      "pays\n",
      "image\n",
      "fabric\n",
      "her\n",
      "area\n",
      "weekend\n",
      "xoxo\n",
      "transform\n",
      "lol\n",
      "hardships\n",
      "stamina\n",
      "stuck\n",
      "lot\n",
      "forward\n",
      "bored\n",
      "head\n",
      "buy\n",
      "form\n",
      "younger\n",
      "feminine\n",
      "hear\n",
      "hmmmmmmm\n",
      "flaky\n",
      "line\n",
      "trying\n",
      "bud\n",
      "maturity\n",
      "peeking\n",
      "similar\n",
      "clear\n",
      "affection\n",
      "trim\n",
      "right\n",
      "politics\n",
      "again\n",
      "variety\n",
      "lately\n",
      "power\n",
      "club\n",
      "details\n",
      "bassein\n",
      "dresses\n",
      "nice\n",
      "picture\n",
      "polite\n",
      "kinda\n",
      "spouse\n",
      "friends\n",
      "hubby\n",
      "assets\n",
      "required\n",
      "toozya\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
